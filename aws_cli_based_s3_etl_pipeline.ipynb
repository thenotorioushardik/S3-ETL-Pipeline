{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import awswrangler as wr\n",
    "\n",
    "# Set AWS credentials\n",
    "os.environ.update({\n",
    "    \"AWS_ACCESS_KEY_ID\": \"<YOUR_AWS_ACCESS_KEY_ID>\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"<YOUR_AWS_SECRET_ACCESS_KEY>\",\n",
    "    \"AWS_DEFAULT_REGION\": \"<YOUR_AWS_DEFAULT_REGION>\"\n",
    "})\n",
    "\n",
    "# Initialize Boto3 client\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas display options\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of a folder\n",
    "\n",
    "def list_folder_contents(bucket, folder):\n",
    "    result = subprocess.run(\n",
    "        [\"aws\", \"s3\", \"ls\", f\"s3://{bucket}/{folder}\", \"--recursive\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return pd.DataFrame(columns=[\"File Name\", \"Size (Bytes)\"])\n",
    "\n",
    "    return pd.DataFrame([\n",
    "        {\"File Name\": \" \".join(line.split()[3:]), \"Size (Bytes)\": int(line.split()[2])}\n",
    "        for line in result.stdout.strip().split(\"\\n\") if len(line.split()) >= 4\n",
    "    ])\n",
    "\n",
    "# Run and display\n",
    "bucket_name = \"<YOUR_BUCKET_NAME>\"\n",
    "folder_path = \"<YOUR_FOLDER_PATH>/\"\n",
    "\n",
    "df = list_folder_contents(bucket_name, folder_path)\n",
    "print(df.to_string(index=False) if not df.empty else \"Folder is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file dynamically based on extension\n",
    "\n",
    "def read_s3_file(bucket, path):\n",
    "    full_path = f\"s3://{bucket}/{path}\"\n",
    "\n",
    "    # Read CSV or TSV file\n",
    "    if path.endswith(('.csv', '.tsv')):\n",
    "        return dd.read_csv(full_path, sep='\\t' if path.endswith('.tsv') else ',', dtype=str, low_memory=False)\n",
    "\n",
    "    # Read Excel file\n",
    "    if path.endswith(('.xlsx', '.xls')):\n",
    "        df = wr.s3.read_excel(full_path, sheet_name=0, dtype=str)\n",
    "        return dd.from_pandas(df, npartitions=10)  # Convert Pandas DF to Dask DF for scalability\n",
    "\n",
    "    raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def process_data(df):\n",
    "    inferred_dtypes = df.head(10000).convert_dtypes().dtypes.to_dict()  # Sampling 10K rows for better inference\n",
    "    return df.map_partitions(lambda part: part.astype(inferred_dtypes, errors='ignore'))\n",
    "\n",
    "# Usage\n",
    "bucket_name = \"<YOUR_BUCKET_NAME>\"\n",
    "s3_file_path = \"<YOUR_S3_FILE_PATH>\"\n",
    "\n",
    "df = process_data(read_s3_file(bucket_name, s3_file_path))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file/folder from S3 and save locally\n",
    "\n",
    "bucket_name = \"<YOUR_BUCKET_NAME>\"\n",
    "s3_path = \"<YOUR_S3_PATH>\"  # Ensure it ends with '/' if it's a folder\n",
    "local_path = \"<YOUR_LOCAL_PATH>\"\n",
    "\n",
    "# Set the AWS_MAX_CONCURRENT_REQUESTS environment variable for parallelism\n",
    "os.environ[\"AWS_MAX_CONCURRENT_REQUESTS\"] = str(10)  # Set concurrent requests\n",
    "\n",
    "# AWS CLI command\n",
    "command = [\n",
    "    \"aws\", \"s3\", \"sync\" if s3_path.endswith('/') else \"cp\",  # Use sync for folder, cp for file\n",
    "    f\"s3://{bucket_name}/{s3_path}\",\n",
    "    local_path,\n",
    "    \"--only-show-errors\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "    print(\"Download complete.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error downloading:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file/folder to S3\n",
    "\n",
    "def upload_to_s3(local_path, bucket_name, s3_path, num_processes=10):\n",
    "    local_path = os.path.abspath(local_path)\n",
    "\n",
    "    # Set AWS CLI to use multipart upload for large files\n",
    "    os.environ[\"AWS_MAX_CONCURRENT_REQUESTS\"] = str(num_processes)\n",
    "\n",
    "    # Check if the local path is a file or a directory\n",
    "    if os.path.isfile(local_path):\n",
    "        command = f'aws s3 cp \"{local_path}\" s3://{bucket_name}/{s3_path}{os.path.basename(local_path)}'\n",
    "    elif os.path.isdir(local_path):\n",
    "        # If it's a directory, use find to list files and upload them\n",
    "        command = (\n",
    "            f'cd \"{local_path}\" && find . -type f | sed \"s|^./||\" | '\n",
    "            f'xargs -P {num_processes} -I {{}} aws s3 cp \"{{}}\" s3://{bucket_name}/{s3_path}{{}}'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Invalid path provided. Please provide a valid file or directory.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(\"Upload completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Usage\n",
    "bucket_name = \"<YOUR_BUCKET_NAME>\"\n",
    "s3_path = \"<YOUR_S3_PATH>\"\n",
    "local_path = \"<YOUR_LOCAL_PATH>\"\n",
    "\n",
    "upload_to_s3(local_path, bucket_name, s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deleting file/folder\n",
    "\n",
    "def delete_from_s3(bucket_name, s3_path):\n",
    "    if s3_path.endswith('/'):\n",
    "        command = f'aws s3 rm \"s3://{bucket_name}/{s3_path}\" --recursive'\n",
    "        print(f\"Deleting folder: s3://{bucket_name}/{s3_path}\")\n",
    "    else:\n",
    "        command = f'aws s3 rm \"s3://{bucket_name}/{s3_path}\"'\n",
    "        print(f\"Deleting file: s3://{bucket_name}/{s3_path}\")\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(f\"Successfully deleted {s3_path} from S3.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while deleting {s3_path} from S3: {e}\")\n",
    "\n",
    "# Usage\n",
    "bucket_name = \"<YOUR_BUCKET_NAME>\"\n",
    "s3_path = \"<YOUR_S3_PATH>\"\n",
    "\n",
    "delete_from_s3(bucket_name, s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
